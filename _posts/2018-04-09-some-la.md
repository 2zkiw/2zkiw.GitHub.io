---
layout: post
title: "Day 3: Some Linear Algebra.."
date: 2018-04-09
categories: "Linear Algebra.."
tags: ["Linear Algebra"]
description: "Some Linear Algebra..by B.Mu 2018/04/09"
header-img: "img/green.jpg"
---
# DIMENSION THEOREM

If V is a finite-dimensional vector space and T: V $\rightarrow$ W is a linear transformation, then 

<center>
dim(Ker(T)) + dim(Im(T)) = dim(V)
</center>

### note:

$\bullet$ The ***inverse image/preimage*** of a vector $\vec{w}$ under the 
transformation T is the set 
<center>
$T^{-1}$ ({$\vec{w}$}) = {$\vec{v}$ $\in$ V $\mid$ T($\vec{v}$) = $\vec{w}$}
</center>

$\bullet$ ***Special Case***: 

- If $\vec{w}$ = 0, 

<center>
$T^{-1}$ ({$\vec{w}$}) = $T^{-1}$ ({$\vec{0}$}) = Ker(T)
</center>

- If $\vec{w}$ $\notin$ Im(T), $T^{-1}$ ({$\vec{w}$}) = $\emptyset$.

$\bullet$ If $\vec{w}$ $\notin$ Im(T), $\vec{v_{1}}$ $\in$ $T^{-1}$({$\vec{w}$}), 

then T($\vec{v_{1}}$) = $\vec{w}$. 

If $\vec{v_{2}}$ is some other element of $T^{-1}$({$\vec{w}$}),

then T($\vec{v_{2}}$) = $\vec{w}$ as well. 

T($\vec{v_{1}}$) - T($\vec{v_{2}}$) = T($\vec{v_{1}}$ - $\vec{v_{2}}$) = $\vec{0}$

$\vec{v_{1}}$ - $\vec{v_{2}}$ $\in$ Ker(T).

### Proposition

$\bullet$ Let T: V $\rightarrow$ W be a linear transformation, and let $\vec{w}$
$\in$ Im(T). Let $\vec{v_{1}}$ be any fixed vector with T($\vec{v_{1}}$) = $\vec{w}$.
Then every vector $\vec{v_{2}}$ $\in$ $T^{-1}$({$\vec{w}$}) can be written uniquely
as ***$\vec{v_{2}}$ = $\vec{v_{1}}$ + $\vec{u}$***, where ***$\vec{u}$ $\in$ Ker(T).***

### Proposition (Half is good enough)

$\bullet$ Let ***dim(V) = dim(W)***. A linear transformation T: v $\rightarrow$ W is 
***injective*** <=> T is ***surjective***.

### Fact

$\bullet$ $T^{-1}$({$\vec{w}$}) is a ***subspace*** of V <=> ***$\vec{w}$ = $\vec{0}$***

- Proof:

$\leftarrow$ :

If $T^{-1}$({$\vec{w}$}) is empty, it cannot be a subspace.

If $T^{-1}$({$\vec{w}$}) is not empty, let $\vec{v}$ $\in$ $T^{-1}$({$\vec{w}$}),

then T(2$\vec{v}$) = 2T($\vec{v}$) = 2 $\vec{w}$ $\neq$ $\vec{w}$ because 
$\vec{w}$ $\neq$ $\vec{0}$.

Therefore, 2$\vec{v}$ $\notin$ $T^{-1}$({$\vec{w}$}), and so $T^{-1}$({$\vec{w}$}) 
is not closed under linear combinations. 

$T^{-1}$({$\vec{w}$}) is not a subspace.

$\rightarrow$ :

$\vec{0}$ $\in$ $T^{-1}$({$\vec{0}$}), so $T^{-1}$({$\vec{0}$}) is non-empty.

Let $\vec{v_{1}}$, $\vec{v_{2}}$ $\in$ $T^{-1}$({$\vec{0}$}), a $\in$ V.

Then T($\vec{v_{1}}$ + $\vec{v_{2}}$) = T($\vec{v_{1}}$) + T($\vec{v_{2}}$) = 
$\vec{0}$ + $\vec{0}$ = $\vec{0}$,

so $\vec{v_{1}}$ + $\vec{v_{2}}$ $\in$ $T^{-1}$({$\vec{0}$}).

T(a$\vec{v_{1}}$) = aT($\vec{v_{1}}$) = a $\vec{0}$ = $\vec{0}$,

so a $\vec{v_{1}}$ $\in$ V, 

so $T^{-1}$({$\vec{0}$}) is closed under linear combinations.

Therefore, $T^{-1}$({$\vec{0}$}) is a subspace of V.

# ISOMORPHISM

Let T: V $\rightarrow$ W be a linear transformation. 

T is ***invertible*** if there exists a linear transformation S: W $\rightarrow$ V 

such that 
<center>
S $\circ$ T is the 
identity transformation on V
</center>

and 

<center>
T $\circ$ S is the identity transformation on W. 
</center>

i.e. ST($\vec{v}$) = $\vec{v}$ for all $\vec{v}$ $\in$ V and TS($\vec{w}$) = 
$\vec{w}$ for all $\vec{w}$ $\in$ W.

T is called an ***isomorphism***.

V and W are isomorphic vector spaces.

The inverse of T is denoted by

$$T^{-1}$$.

If we want to show S is the inverse transformation of T, we need to verify:
<center>
$T$$T^{-1}$ = $I_{w}$, $T^{-1}$$T$ = $I_{v}$.
</center>

### Fact

$\bullet$ If T is not invertible, there is no transformation $T^{-1}$, 
but it make sense to consider the set ***$T^{-1}$({$\vec{w}$}) $\subset$ V.***

### Propositon

$\bullet$ A linear transformation T: V $\rightarrow$ W has an inverse linear 
transformation S <=> T is ***injective and surjective***

- Proof:

$\rightarrow$ :

If T has an inverse linear transformation S, and T($\vec{v_{1}}$) = 
T($\vec{v_{2}}$),

then $\vec{v_{1}}$ = S(T($\vec{v_{1}}$)) = S(T($\vec{v_{2}}$)) = $\vec{v_{2}}$,

so that T is injective.

If $\vec{w}$ $\in$ W, then for $\vec{v}$ = S($\vec{w}$), 

T($\vec{v}$) = T(S($\vec{w}$)) = $\vec{w}$,

so that T is surjective.

$\leftarrow$ :

Let $\vec{w_{1}}$, $\vec{w_{2}}$ $\in$ W, a,b $\in$ R. 

S($\vec{w_{1}}$) = $\vec{v_{1}}$, S($\vec{w_{2}}$) = $\vec{v_{2}}$, by definition.

$\vec{v_{1}}$, $\vec{v_{2}}$ are the unique vectors s.t T($\vec{v_{1}}$) = 
$\vec{w_{1}}$ and T($\vec{v_{2}}$) = $\vec{w_{2}}$ since T is injective and 
surjective.

T(S(a$\vec{w_{1}}$) + b$\vec{w_{2}}$) = a$\vec{w_{1}}$) + b$\vec{w_{2}}$ by 
definition.

But T(a$\vec{v_{1}}$ + b$\vec{v_{2}}$) = aT($\vec{v_{1}}$) + bT($\vec{v_{2}}$) 
= a$\vec{w_{1}}$ + b$\vec{w_{2}}$.

Thus, S(a$\vec{w_{1}}$ + b$\vec{w_{2}}$ = a$\vec{v_{1}}$ + b$\vec{v_{2}}$ = 
aS($\vec{w_{1}}$) + bS($\vec{w_{2}}$) since T is injective and surjective.


### Proposition

$\bullet$ If V and W are finite-dimensional vector spaces, then there is an 
***isomorphism*** T: V $\rightarrow$ W <=> dim(V) = dim(W).

- Proof:

$\rightarrow$:

If T is an isomorphism, T is injective and surjective, so that dim(Ker(T)) = 
dim({$\vec{0}$}) = 0 and dim(Im(T)) = dim(W).

Thus, dim(V) = dim(W) by the dimension theorem.

$\leftarrow$:

If dim(V) = dim(W), let $\alpha$ = {$\vec{v_{1}}$, ..., $\vec{v_{n}}$} be a basis
for V and $\beta$ = {$\vec{w_{1}}$, ..., $\vec{w_{n}}$} be a basis for W.

Define T to be the linear transformatio from V to W with T($\vec{v_{i}}$) = 
$\vec{w_{i}}$, 1 $\leq$ i $\leq$ n.

T is uniquely determined by its values on the members of  basis $\alpha$
of V.

If T($a_{1}$ $\vec{v_{1}}$ + ... + $a_{n}$ $\vec{v_{n}}$) = $\vec{0}$,

then $a_{1}$ $\vec{w_{1}}$ + ... + $a_{n}$ $\vec{w_{n}}$ = $\vec{0}$.

$a_{1}$ = ... = $a_{n}$ = 0 because $\vec{w_{1}}$, ..., $\vec{w_{n}}$ are a basis.

Thus, Ker(T) = {$\vec{0}$}. T is injective.

T is also surjective because dim(V) = dim(W).

### Fact

$\bullet$ If 0 is an ***eigenvalue*** of T, i.e. there exists a non-zero vector $\vec{x}$ in V 
s.t. T($\vec{x}$) = $\vec{0}$, Ker(T) = $E_{0}$ $\neq$ {0}, T is not injective. Therefore, T is not invertible.

### Invertible matrix:

- A $\in$ $M_{nxn}$ (R), **det(A) = 0** $\Longrightarrow$ A is **not invertible**

- T is not invertible, $[T]_{\alpha}^{\alpha}$ is not invertible.

# Diagonalization

A Linear Operator T: V $\rightarrow$ V is ***diagonalizable*** if there exists 
a basis $\beta$ of V consisting ***only*** of ***eigenvectors*** of T such that 
$[T]_{\beta}^{\beta}$ is ***diagonal***. 

$\bullet$ **Why** diagonalize? Some transformations only stretch or compress vectors..NOT 
ROTATE !

### Proposition

$\bullet$ T: V $\rightarrow$ V is ***diagonalizable*** if and only if, for any basis 
$\alpha$ of V, the matrix $[T]_{\alpha}^{\alpha}$ is similar to a ***diagonal*** 
matrix.

$\bullet$ T: V $\rightarrow$ V is ***diagonalizable*** if and only if,**dim $E_{\lambda i}$ 
= $m_{\lambda i}$ for $\forall$ i**

- note that If $\lambda$ is an eigenvalue of T with multiplicity m, 
	then **1 $\leq$ dim $E_{\lambda}$ $\leq$ m**, this means, eigenvalue 
	$\lambda$ cannot cause the failure of diagonalization.

### Special Case: 

<center>
	Eigenvectors corresponding to distinct eigenvalues are linearly independent.
</center>

<center>
	$\Longrightarrow$ If T has dim V distinct eigenvalues, then T is 
	diagonalizable.
</center>

### Fact: 
<center>
	A is diagonalizable $\Longrightarrow$ $A^{n}$ is diagonalizable.
	(Converse is not necessarily true in general.)
</center>

# Invariant

T: V $\rightarrow$ V is a linear operator. A subspace W of V is invariant 
under T if 
<center>
	T(W) $\subset$ W. (i.e. $\forall$ $\vec{w}$ $\in$ W, T($\vec{w}$) $\in$ W.)
</center>

# Fact:

<center>
	Let V be a vector space over field F(F = R or C).
</center>
<center>
	If the characteristic polynomial of T has dim V roots in F,
</center>
<center>
	then T is triangularizable.
</center>


$\bullet$ e.g. V, {0}, Ker(T), Im(T), $E_{\lambda}$

# Jordan Canonical Form

$\bullet$ ***Ker(T - $\lambda$ I) $\neq$ null($[T]_{\alpha}^{\alpha}$ - $\lambda$ I)***

- Vectors in ***Ker(T)*** are vectors in codomain(i.e. vector space V) of T.

- Vectors in ***null($[T]^{\alpha}_{\alpha}$ - $\lambda$ I)*** are coordinates 
of vectors in Ker(T) corresponding to basis $\alpha$.

- DON'T FORGET TO **TRANSFOR BACK** TO VECTORS IN V ÔºÅ

- DON'T USE COORDINATE VECTORS AS CANONICAL BASIS !

# Similarity

The matrices A and B are called ***similar*** if there is an invertible 
matrix s.t. 
<center>
	$A$ = $P$$B$$P^{-1}$.
</center>

### Property

$\bullet$ Similar matrices represent the same linear transformation but in 
(possibly) different bases.

- The converse is not true in general.

- If T is a linear transformation from finite-dimensional vector space V to V,

then two matrices represent the same transformation T are similar.

### Fact
<center>
	$A$ = $P$$D$$P^{-1}$ $\Longrightarrow$ $A^{n}$ = $P$$D^{n}$$P^{-1}$
</center>

# Some Remark..

### Some Transformation

$\bullet$ An operator T is called a ***projection*** if it satisfies 
<center>
$T^{2}$ = T.
</center>

- If T is a projection on V, then 

<center>
V = Ker(T) $\bigoplus$ Im(T)
</center>

- If T is a projection on V, its only possible ***eigenvalues*** are 0 and 1.

- If T is a projection on V and V is finite-dimensional, then T is 
***diagonalizable***.

$\bullet$ An nxn matrix A with entries from field F satisfying $A^{3}$ = A. 
Let V = $F^{n}$.

- 
<center>
V = $E_{0}$ $\bigoplus$ $E_{1}$ $\bigoplus$ $E_{-1}$
</center>

- A is ***diagonalizable***.

$\bullet$ Let T:V $\rightarrow$ V be a linear mapping s.t. $T^{2}$ = I, T is called an 
***involution***.

- The only possible ***eigenvalues*** of T are +1 and -1.

- 
<center>
V = $E_{1}$ $\bigoplus$ $E_{-1}$.
</center>

- T is ***diagonalizable*** (following the result above since A $\cdot$ $A^{2}$
 = A $\cdot$ I).

- e.g. T: $M_{nxn}$ (R) $\rightarrow$ $M_{nxn}$ (R) be the transpose 
mapping T(A) = $A^{T}$. In this case, $E_{+1}$ is the set of all nxn ***symmetric***
matrices, $E_{-1}$ is the set of all nxn ***skew symmetric*** matrices.

### Direct Sum

$\bullet$ How to prove V = $E_{1}$ $\bigoplus$ $E_{-1}$ ?

- (1) WTS the only ***eigenvalues*** are +1 and -1

- (2) WTS 
<center>
$E_{1}$ $\bigcap$ $E_{-1}$ = {0}
</center>

- (3) WTS 
<center>
V = $E_{1}$ + $E_{-1}$
</center>

- Hint: 
<center>
For $\forall$ $\vec{x}$ $in$ V, $\vec{x}$ = $\frac{1}{2}$ 
($\vec{x}$ + T($\vec{x}$)) + $\frac{1}{2}$ ($\vec{x}$ - T($\vec{x}$)).
</center>

