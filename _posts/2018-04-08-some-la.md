---
layout: post
title: "Day 2: Some Linear Algebra.."
date: 2018-04-08
categories: "Linear Algebra.."
tags: []
description: "Some Linear Algebra..by B.Mu from 2018/04/08"
header-img: "img/green.jpg"
---
# Some Linear Algebra..

![avatar](/img/la1.jpg)

# Change of Basis

![avatar](/img/la2.png)

# Note:

$\star$ Let T: V $\rightarrow$ V is a linear transformation, 
V is a finite-dimensional vector space.

### *Jordan Canonical Form*

- **Ker(T - $\lambda$ I) $\neq$ null($[T]_{\alpha}^{\alpha}$ - $\lambda$ I)**

- Vectors in Ker(T) are vectors in codomain(i.e. vector space V) of T.

- Vectors in null($[T]^{\alpha}_{\alpha}$ - $\lambda$ I) are coordinates 
of vectors in Ker(T) corresponding to basis $\alpha$.

- DON'T FORGET TO **TRANSFOR BACK** TO VECTORS IN V ÔºÅ

- DON'T USE COORDINATE VECTORS AS CANONICAL BASIS !

### *Similarity*

- The matrices A and B are called similar if there is an invertible 
matrix s.t. **A = PB$P^{-1}$**.

- Similar matrices  represent the same linear transformation but in 
(possibly) different bases.

- *Fact*

<center>
	**A = PD$P^{-1}$ $\Longrightarrow$ $A^{n}$ = P$D^{n}$$P^{-1}$**
</center>

### *Diagonalization*

- Why diagonalize? Some transformations only stretch or compress vectors..NOT 
ROTATE !

- A Linear Operator T: V $\rightarrow$ V is diagonalizable if there exists 
a basis $\beta$ of V consisting only of eigenvectors of T such that 
$[T]_{\beta}^{\beta}$ is diagonal. 

- T: V $\rightarrow$ V is diagonalizable if and only if, for any basis 
$\alpha$ of V, the matrix $[T]_{\alpha}^{\alpha}$.

- **dim $E_{\lambda i}$ = $m_{\lambda i}$ for $\forall$ i**

<center>
	note that If $\lambda$ is an eigenvalue of T with multiplicity m, 
	then **1 $\leq$ dim $E_{\lambda}$ $leq$ m**, this means, eigenvalue 
	$\lambda$ cannot cause the failure of diagonalization.
</center>

- ***Special Case***: 

<center>
Eigenvectors corresponding to distinct eigenvalues 
are linearly independent.
</center>

<center>
	$\Longrightarrow$ If T has dim V distinct eigenvalues, then T is 
	diagonalizable.
</center>

- ***Fact***: A is diagonalizable $\Longrightarrow$ $A^{n}$ is diagonalizable.
(Converse is not necessarily true in general.)


### *Invertibility*

- A $\in$ $M_{nxn}$ (R), **det(A) = 0** $\Longrightarrow$ A is **not invertible**

- If 0 is an eigenvalue of T, i.e. there exists a non-zero vector $\vec{x}$ in V 
s.t. T($\vec{x}$) = $\vec{0}$, Ker(T) = $E_{0}$ $\neq$ {0}, T is not injective, 
T is not invertible. $[T]_{\alpha}^{\alpha}$ $\cdot$ $[\vec{x}]_{\alpha}$ = 0
has non-trivial solution, columns of $[T]_{\alpha}^{\alpha}$ are not linearly
independent, $[T]_{\alpha}^{\alpha}$ is not invertible.

### *Invariant*

- T: V $\rightarrow$ V is a linear operator. A subspace W of V is invariant under T if T(W) $\subset$ W. (i.e. $\forall$ $\vec{w}$ $\in$ W, T($\vec{w}$) $\in$ W.)

- *Fact*:

<center>
	Let V be a vector space over field F(F = R or C). If the characteristic 
	polynomial of T has dim V roots in F, then T is triangularizable.
</center>

- e.g. V, {0}, Ker(T), Im(T), $E_{\lambda}$

### *Isomorphism*

- $T^{-1} ({\vec{0}})$ $\neq$ $T^{-1} (\vec{0})$

### *Some Transformation*

$\bullet$ An operator T is called a projection if it satisfies 
$T^{2}$ = T.

- If T is a projection on V, then V = Ker(T) $\bigoplus$ Im(T)

- If T is a projection on V, its only possible eigenvalues are 0 and 1.

- If T is a projection on V and V is finite-dimensional, then T is 
diagonalizable.

$\bullet$ An nxn matrix A with entries from field F satisfying $A^{3}$ = A. 
Let V = $F^{n}$.

- V = $E_{0}$ $\bigoplus$ $E_{1}$ $\bigoplus$ $E_{-1}$

- A is diagonalizable.

$\bullet$ Let T:V $\rightarrow$ V be a linear mapping s.t. $T^{2}$ = I, T is called an 
involution.

- The only possible eigenvalues of T are +1 and -1.

- V = $E_{1}$ $\bigoplus$ $E_{-1}$.

- T is diagonalizable(following the result above since A $\cdot$ $A^{2}$
 = A $\cdot$ I).

- e.g. T: $M_{nxn}$ (R) $\rightarrow$ $M_{nxn}$ (R) be the transpose 
mapping T(A) = $A^{T}$. In this case, $E_{+1}$ is the set of all nxn symmetric 
matrices, $E_{-1}$ is the set of all nxn skew symmetric matrices.

### *Direct Sum*

$\bullet$ How to prove V = $E_{1}$ $\bigoplus$ $E_{-1}$ ?

- 1. WTS the only eigenvalues are +1 and -1

- 2. WTS $E_{1}$ $\bigcap$ $E_{-1}$ = {0}

- 3. WTS V = $E_{1}$ + $E_{-1}$

- Hint: For $\forall$ $\vec{x}$ $in$ V, $\vec{x}$ = $\frac{1}{2}$ 
($\vec{x}$ + T($\vec{x}$)) + $\frac{1}{2}$ ($\vec{x}$ - T($\vec{x}$)).










