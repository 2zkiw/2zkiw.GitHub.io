---
layout: post
title: "Day 2: Some Linear Algebra.."
date: 2018-04-08
categories: "Linear Algebra.."
tags: []
description: "Some Linear Algebra..by B.Mu from 2018/04/08"
header-img: "img/green.jpg"
---
# Some Linear Algebra..

![avatar](/img/la1.jpg)

# Change of Basis

![avatar](/img/la2.png)

# Note:

$\star$ Let T: V $\rightarrow$ V is a linear transformation, 
V is a finite-dimensional vector space.

### *Jordan Canonical Form*

$\bullet$ **Ker(T - $\lambda$ I) $\neq$ null($[T]_{\alpha}^{\alpha}$ - $\lambda$ I)**

- Vectors in Ker(T) are vectors in codomain(i.e. vector space V) of T.

- Vectors in null($[T]^{\alpha}_{\alpha}$ - $\lambda$ I) are coordinates 
of vectors in Ker(T) corresponding to basis $\alpha$.

- DON'T FORGET TO **TRANSFOR BACK** TO VECTORS IN V ÔºÅ

- DON'T USE COORDINATE VECTORS AS CANONICAL BASIS !

### *Similarity*

- The matrices A and B are called similar if there is an invertible 
matrix s.t. 
<center>
	$A$ = $P$$B$$P^{-1}$.
</center>

- Similar matrices  represent the same linear transformation but in 
(possibly) different bases.

- ***Fact***
<center>
	$A$ = $P$$D$$P^{-1}$ $\Longrightarrow$ $A^{n}$ = $P$$D^{n}$$P^{-1}$
</center>

### *Diagonalization*

- **Why** diagonalize? Some transformations only stretch or compress vectors..NOT 
ROTATE !

- A Linear Operator T: V $\rightarrow$ V is diagonalizable if there exists 
a basis $\beta$ of V consisting only of eigenvectors of T such that 
$[T]_{\beta}^{\beta}$ is diagonal. 

- T: V $\rightarrow$ V is diagonalizable if and only if, for any basis 
$\alpha$ of V, the matrix $[T]_{\alpha}^{\alpha}$ is similar to a diagonal 
matrix.

- T: V $\rightarrow$ V is diagonalizable if and only if,**dim $E_{\lambda i}$ 
= $m_{\lambda i}$ for $\forall$ i**

- note that If $\lambda$ is an eigenvalue of T with multiplicity m, 
	then **1 $\leq$ dim $E_{\lambda}$ $\leq$ m**, this means, eigenvalue 
	$\lambda$ cannot cause the failure of diagonalization.

- ***Special Case***: 

<center>
	Eigenvectors corresponding to distinct eigenvalues are linearly independent.
</center>

<center>
	$\Longrightarrow$ If T has dim V distinct eigenvalues, then T is 
	diagonalizable.
</center>

- ***Fact***: 
<center>
	A is diagonalizable $\Longrightarrow$ $A^{n}$ is diagonalizable.
	(Converse is not necessarily true in general.)
</center>


### *Invertibility*

- A $\in$ $M_{nxn}$ (R), **det(A) = 0** $\Longrightarrow$ A is **not invertible**

- If 0 is an eigenvalue of T, i.e. there exists a non-zero vector $\vec{x}$ in V 
s.t. T($\vec{x}$) = $\vec{0}$, Ker(T) = $E_{0}$ $\neq$ {0}, T is not injective, 
T is not invertible. Also, $[T]_{\alpha}^{\alpha}$ is not invertible.

### *Invariant*

- T: V $\rightarrow$ V is a linear operator. A subspace W of V is invariant 
under T if 
<center>
	T(W) $\subset$ W. (i.e. $\forall$ $\vec{w}$ $\in$ W, T($\vec{w}$) $\in$ W.)
</center>

- ***Fact***:

<center>
	Let V be a vector space over field F(F = R or C).
</center>
<center>
	If the characteristic polynomial of T has dim V roots in F,
</center>
<center>
	then T is triangularizable.
</center>


- e.g. V, {0}, Ker(T), Im(T), $E_{\lambda}$

### *Isomorphism*

- $T^{-1} ({\vec{0}})$ $\neq$ $T^{-1} (\vec{0})$

### *Some Transformation*

$\bullet$ An operator T is called a projection if it satisfies 
$T^{2}$ = T.

- If T is a projection on V, then V = Ker(T) $\bigoplus$ Im(T)

- If T is a projection on V, its only possible eigenvalues are 0 and 1.

- If T is a projection on V and V is finite-dimensional, then T is 
diagonalizable.

$\bullet$ An nxn matrix A with entries from field F satisfying $A^{3}$ = A. 
Let V = $F^{n}$.

- V = $E_{0}$ $\bigoplus$ $E_{1}$ $\bigoplus$ $E_{-1}$

- A is diagonalizable.

$\bullet$ Let T:V $\rightarrow$ V be a linear mapping s.t. $T^{2}$ = I, T is called an 
involution.

- The only possible eigenvalues of T are +1 and -1.

- V = $E_{1}$ $\bigoplus$ $E_{-1}$.

- T is diagonalizable(following the result above since A $\cdot$ $A^{2}$
 = A $\cdot$ I).

- e.g. T: $M_{nxn}$ (R) $\rightarrow$ $M_{nxn}$ (R) be the transpose 
mapping T(A) = $A^{T}$. In this case, $E_{+1}$ is the set of all nxn symmetric 
matrices, $E_{-1}$ is the set of all nxn skew symmetric matrices.

### *Direct Sum*

$\bullet$ How to prove V = $E_{1}$ $\bigoplus$ $E_{-1}$ ?

- (1) WTS the only eigenvalues are +1 and -1

- (2) WTS $E_{1}$ $\bigcap$ $E_{-1}$ = {0}

- (3) WTS V = $E_{1}$ + $E_{-1}$

- Hint: For $\forall$ $\vec{x}$ $in$ V, $\vec{x}$ = $\frac{1}{2}$ 
($\vec{x}$ + T($\vec{x}$)) + $\frac{1}{2}$ ($\vec{x}$ - T($\vec{x}$)).










